//CS 35L - Professor Eggert
//Lab 4 - TA Isha Verma
//Project 2 - Shell Scripting
//log2.log

- To begin this lab, I signed onto the SEASNet Server via my terminal:

$ ssh [username]@lnxsrv09.seas.ucla.edu
[password]

- Next, I checked to make sure that I was using the standard C or POSIX 
locale:

$ locale

- As, LC_CTYPE="C" or LC_CTYPE="POSIX” were not given as output, I used the
following shell command to make sure that locale outputs the right thing:

$ export LC_ALL=‘C’

- Next, I crated a file called words that was sorted using the sort command.
I sorted the original words file and then placed the sorted version in 
another words file on my SEASNet Desktop:

$ cd /usr/share/dict
$ sort words > ~/Desktop/words

- The text file contains the HTML of this assignment was then put into a 
text file using the wget command, and then changed from a .html to a .txt
file via the mv command:

$ wget http://http://web.cs.ucla.edu/classes/winter17/cs35L/assign
/assign2.html
$ mv assign2.html assign2.txt

First of all, tr is a Linux command that is used to translate characters 
into other characters or to delete them. The syntax of tr => $ tr [options] 
set1 [set2]. The first set is a set of the characters that are to be 
translated, and the second set consists of what to replace set1 with. 
For these examples, set1 consists of all of the uppercase and lowercase 
letters (i.e. all letter characters), while set2 consists of the newline 
character.

The -c option stands for complement, and it is used to complement set1 (i.e.
the characters not in set1) with that of set2. The -s option is used as a 
squeeze operation, which replaces a sequence of the repeated characters in
set1 with a single instance of that such character. 

The following commands were run using the text file as standard input, with
a description of what they do shown below:

(1) tr -c 'A-Za-z' '[\n*]'

Given the description of tr above, this command takes the complement of all
of the letter characters, which is all non-letter characters, and replaces
these non-letter characters with a newline character. For example, 
characters such as; ‘<‘, ‘!’, ‘ ‘, and [digits] are all replaced by 
a newline character. Thus, this command outputs the strings of characters 
with the appropriate number of newline characters in between them.

(2) tr -cs 'A-Za-z' '[\n*]'

This command builds off of the first command, as it makes use of the -s 
option to remove sequences of repeated characters with a single instance of
them. In this case, it is replacing the sequences of multiple newline 
characters (i.e. the vertical spaces in between the words) with a single 
instance of them. Thus, it outputs all of the words, or strings of letter
characters once per line, and one right after the other.

(3) tr -cs 'A-Za-z' '[\n*]' | sort

This command builds off of the previous command, as it now uses a pipeline
to the sort filter. The sort pipe sorts the contents of the text-file line
by line, numerically and alphabetically according to three such rules:

(a) lines starting with a number appear before lines starting with a letter
(b) lines starting with a letter earlier in the alphabet appear before 
    letters that appear after it (i.e. letter a comes before letter b)
(c) lines starting with uppercase letters appear before lines starting with
    lowercase letters

Thus, this command outputs the same thing as the previous command; however,
it now outputs them in a sorted order per the rules described above.

(4) tr -cs 'A-Za-z' '[\n*]' | sort -u

This command builds off of the previous command, as it now uses one of the
sort options (i.e. the -u option), which stands for “unique”, and outputs 
only the first instance of a given run (i.e. if there are multiple letter
A’s, then it will output only one A). Therefore, it produces a sorted list
like the previous command does, but it removes the duplicate lines.

(5) tr -cs 'A-Za-z' '[\n*]' | sort -u | comm - words

This command builds off of the previous command, as it adds a new command
‘comm’. The comm command compares two sorted files line by line, and then
can select/reject lines common to the two files. To build off of that, it 
will output three columns:

(a) column 1: unique to file 1
(b) column 2: unique to file 2
(c) column 3: lines unique to both files

Thus, it outputs a comparison of the sorted file that was created earlier 
with that of the original words file into the three columns described above.
As shown by the output, column 2 has a considerably larger number of unique
lines from file 1 as it was the original, and due to the steps to alter
file 1 by the previous command examples.

(6) tr -cs 'A-Za-z' '[\n*]' | sort -u | comm -23 - words

This command builds off of the previous command, as it now uses an 
additional option for the comm command. This option has three types:

(a) -1: suppress column 1 (i.e. don’t show it)
(b) -2: suppress column 2
(c) -3: suppress column 3

Thus, this command will output what the previous command did, but will 
omit columns 2 and 3, and thus the only output will be lines unique to 
file 1.



- Next, I created a new file called hwords via the touch command:

$ touch hwords

- Then, I copied the “English to Hawaiian” webpage via Wget:

$ wget http://mauimapp.com/moolelo/hwnwdseng.htm

- Next, I created the script for Buildwords as shown below:


#!/bin/sh 

- The first line that is used to tell the kernel which shell to use for 
the script

grep ‘<td>.\{1,|}</td>’ | 

- grep is used to select lines that match one or more patterns specified.
This is used to select lines that start with <td> and end with </td>. The 
characters in between the brackets are used to match one or more of the
preceding characters (and thus there can be any input in between the td’s)

sed ‘1d; n; d’ |

- sed is the stream text editor that is used to replace, insert, or delete
words from text. This command is used to delete all of the odd lines in the
text file and to keep the even lines. This command was done in order to only
focus on the Hawaiian words, and to delete the English words. At first, I
didn’t have this command, and operated on the entire text file. However,
there are some English words that are composed solely of Hawaiian characters,
and thus I was unable to delete these characters with the commands below. To
combat this, I looked for a way to eliminate the English words altogether.

sed ’s/<[^>]*>//g’ | 

- In this case, I am using the substitution operation of 
sed as means to delete the HTML tags of the file. The syntax of this 
operation => sed ’s/[word to replace]/[replacement word]/[options]’. The g
option is used to replace all instances of the corresponding word to replace.
Here, there is no replacement word, and the word to replace will search for
expressions beginning with ‘<‘, followed by 0 or more characters due 
to the ‘*’ wildcard that don’t have a ‘>’ character, and then end in 
the ‘>’ character.

sed ’s/`/‘\’’/g’ | 

- This command is used to replace all instances of the okina with the
apostrophe

tr ‘, ‘ ‘[\n*]’ |

- This command is used to replace all commas and spaces with newlines

sed ‘/n$/d’ |

- This command is used to remove all of the blank newlines

tr ‘[:upper:]’ ‘[:lower:]’ | 

- This command is used to replace all instances of uppercase letters with 
that of their lowercase equivalents

sed ‘/[^p^k^’\’’^m^n^w^l^h^a^e^i^o^u]/d’ |

- This command is used to delete all characters and words that do not 
correspond to the Hawaiian letters

sort -u

- Finally, this command will sort the Hawaiian dictionary and uses the -u
option to remove all duplicates of words

- The script was then executed with the Hawaiian words website as the 
STD input and then this was output to the hwords file:

$ ./buildwords < hwnwdseng.htm > hwords

- Upon first running this command however, the permission was denied. I then
used the chmod command to allow for execute permissions, and then ran it 
again:

$ chmod u+x buildwords


- Next, the last shell command was modified to use the Hawaiian spelling
checker on this webpage. Before that was done, the webpage was downloaded
via wget, and then all of the uppercase letters were changed to lowercase via
tr:

$ wget http://web.cs.ucla.edu/classes/winter17/cs35L/assign/assign2.html
$ tr ‘PKMNWLHAEIOU’ ‘pkmnwlhaeiou’ < assign2.html

- Now, the spell checker was applied to the webpage via the last shell 
command:

$ tr -cs ‘pk’\’’mnwlhaeiou’ '[\n*]' < assign2.html | sort -u | 
comm -23 - hwords | wc -l

- This used the Hawaiian spell checker on the webpage, and found that there
were 199 misspelled words

- To count the number of misspelled English words:

$ tr -cs ‘A-Za-z’ '[\n*]' < assign2.html | sort -u | comm -23 - words | 
wc -l

- It was found that there were 81 misspelled English words


- To find the number of misspelled English words, but not Hawaiian words, 
and vice versa:

$ tr -cs ‘A-Za-z’ '[\n*]' < assign2.html | sort -u | comm -23 - words > 
file1.txt
$ tr -cs ‘pk’\’’mnwlhaeiou’ '[\n*]' < assign2.html | sort -u | 
comm -23 - hwords > file2.txt
$ comm -23 file1.txt file2.txt | wc -l
$ comm -13 file1.txt file2.txt | wc -l

- It was found that there were 76 misspelled English words, but not Hawaiian:
Ex: basedefs, charset, Exp, href, html, etc.
- It was found that there 194 misspelled Hawaiian words, but not English:
Ex: ain, ameln, enien, epla, uppo, etc.



